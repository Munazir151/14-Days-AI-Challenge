# ðŸš€ Databricks 14 Days AI Challenge  
### Lakehouse, Apache Spark & AI-Ready Data Engineering

This repository contains my **hands-on implementation** of the **Databricks 14 Days AI Challenge** â€” a structured learning initiative focused on mastering Databricks Lakehouse fundamentals, Apache Spark analytics, modern data engineering workflows, and preparation for AI/ML solutions.

The objective of this project is to design **production-style data pipelines** on the Databricks Lakehouse platform and apply them to **large-scale e-commerce data ingestion, transformation, and analytics**.

---

## ðŸ“Œ Challenge Overview

Participants explore Databricks and Spark fundamentals, build scalable workflows, and apply advanced transformations while sharing progress publicly.  
The challenge emphasizes **real-world learning through hands-on tasks** on the Databricks platform.

### Key Focus Areas
- Databricks Lakehouse architecture  
- Apache Spark & PySpark fundamentals  
- Data ingestion, transformation, and analytics  
- Delta Lake for reliability and governance  
- Preparation for downstream AI/ML applications  

---

## ðŸ§° Technology Stack

| Component | Purpose |
|---------|--------|
| Databricks | Unified data & AI platform |
| Apache Spark (PySpark) | Distributed data processing |
| Delta Lake | ACID tables & schema enforcement |
| Unity Catalog & Volumes | Governed storage & metadata |
| SQL / Python | Notebook-based workflows |
| GitHub | Version control & collaboration |
| Power BI *(upcoming)* | Downstream analytics |
| AI & ML Pipelines *(upcoming)* | Predictive modeling & features |

---

## ðŸ“Š Data Description

This project works with **large multi-month e-commerce event datasets (~5 GB+)**, enabling realistic ingestion and transformation scenarios.  
Data is organized, processed, and validated using **Databricks Volumes and Delta Lake tables**.

---

## ðŸ’» What Youâ€™ll Find Here

### âœ” Databricks Platform Setup
- Signup & environment configuration  
- Workspace, cluster, and notebook navigation  

### âœ” Apache Spark Analytics
- Spark architecture (Driver, Executors, DAG)  
- DataFrames vs RDDs  
- Lazy evaluation & notebook magic (`%sql`, `%python`, `%fs`)  

### âœ” Advanced Transformations
- Complex joins (inner, outer, left, right)  
- Window functions for running totals & ranking  
- User-Defined Functions (UDFs)  

### âœ” Delta Lake & Governance
- Delta table creation & optimization  
- Schema enforcement & evolution  
- Unity Catalog integration and data governance  

---

## ðŸŽ¯ Challenge Goals
- Build enterprise-grade social/e-commerce data pipelines  
- Learn to manage data at scale using Lakehouse patterns  
- Prepare cleaned datasets for AI & ML tasks  
- Share progress publicly using designated challenge tags  

---

## ðŸ™Œ Acknowledgements

This challenge is organized and supported by:
- **Databricks** â€“ Unified Lakehouse platform provider  
- **Codebasics** â€“ Content guidance & materials  
- **Indian Data Club** â€“ Community host & orchestrator  

Huge thanks for enabling a structured, hands-on, industry-focused learning experience.

---

## ðŸ“Œ Official Challenge Tags

#DatabricksWithIDC #Databricks #IndianDataClub #Codebasics
