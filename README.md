ðŸš€ Databricks 14 Days AI Challenge

Lakehouse, Apache Spark & AI-Ready Data Engineering

This repository contains my hands-on implementation of the Databricks 14 Days AI Challenge â€” a structured learning program focused on mastering Databricks Lakehouse fundamentals, Spark analytics, data engineering workflows, and preparation for AI/ML solutions.

The goal is to build production-style data pipelines on the Databricks Lakehouse platform and apply them to large-scale e-commerce data processing & analytics.

ðŸ“Œ Challenge Overview

Participants explore Databricks and Spark fundamentals, build scalable workflows, and apply advanced transformations while sharing progress publicly. The challenge includes guided lessons, tasks, and real-world learning on the Databricks platform.

Key focus areas:

Databricks Lakehouse architecture

Apache Spark & PySpark essentials

Data ingestion, transformation, and analytics

Delta Lake for reliability and governance

Preparation for downstream AI/ML applications

ðŸ§° Technology Stack
Component	Purpose
Databricks	Unified data & AI platform
Apache Spark (PySpark)	Distributed data processing
Delta Lake	ACID tables, schema enforcement
Unity Catalog & Volumes	Governed storage & metadata
SQL / Python	Notebook workflows
GitHub	Version control & sharing
Power BI (upcoming)	Downstream visualization
ML & AI pipelines (upcoming)	Predictive modeling & features
ðŸ“‚ Repository Structure
databricks-14days-ai-challenge/
â”‚
â”œâ”€â”€ Day01-PlatformSetup/      # Intro to Databricks & workspace
â”œâ”€â”€ Day02-SparkFundamentals/  # Spark architecture & DataFrames
â”œâ”€â”€ Day03-Transformations/    # Joins, window functions, UDFs
â”œâ”€â”€ Day04-DeltaGovernance/    # Delta tables, Unity Catalog
â”œâ”€â”€ ingestion/                # Ingestion notebooks & scripts
â”œâ”€â”€ bronze/                   # Raw/refined tables
â”œâ”€â”€ silver/                   # Cleaned business data
â”œâ”€â”€ gold/                     # Analytics/feature datasets
â”œâ”€â”€ ai_ml/                    # Features for AI & ML
â””â”€â”€ README.md

ðŸ“Š Data Description

This project works with large multi-month e-commerce event datasets (~5 GB+), enabling realistic ingestion and transformation scenarios. Data is organized, processed, and validated using Databricks volumes and Delta Lake tables.

ðŸ’» What Youâ€™ll Find Here
âœ” Databricks Platform Setup

Signup & environment configuration

Workspace, cluster, and notebook navigation

âœ” Apache Spark Analytics

Understanding drivers, executors, DAGs

DataFrames vs RDDs

Lazy evaluation & notebook magic (%sql, %python, %fs)

âœ” Advanced Transformations

Complex joins (inner/outer/left/right)

Window functions for rolling totals & ranking

User-Defined Functions (UDFs)

âœ” Delta Lake & Governance

Delta table creation & optimization

Schema enforcement & evolution

Unity Catalog integration and data governance

ðŸŽ¯ Challenge Goals

Build enterprise-grade social/e-commerce data pipelines

Learn to manage data at scale using Lakehouse patterns

Prepare cleaned datasets for AI & ML tasks

Share progress publicly on social channels using designated tags

ðŸ™Œ Acknowledgements

This challenge is organized and supported by:

Databricks â€“ Unified Lakehouse platform provider

Codebasics â€“ Content guidance & materials

Indian Data Club â€“ Community host & orchestrator

Huge thanks for enabling structured, hands-on learning!

ðŸ“Œ Official Challenge Tags
#DatabricksWithIDC #Databricks #IndianDataClub #Codebasics
