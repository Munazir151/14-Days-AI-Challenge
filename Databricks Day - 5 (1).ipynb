{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081d79e5-7dfe-4f96-aea6-e364a971d8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read CSV\n",
    "oct_df = spark.read.csv(\n",
    "    \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Step 2: Set Delta table path\n",
    "delta_path = \"/Volumes/workspace/ecommerce/delta/ecommerce_events\"\n",
    "\n",
    "# Step 3: Save as Delta table (overwrite if exists)\n",
    "oct_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "# Step 4: Import DeltaTable and Spark functions\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 5: Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Step 6: Deduplicate source to avoid multiple source row errors\n",
    "window_spec = Window.partitionBy(\n",
    "    \"user_session\",\n",
    "    \"event_time\",\n",
    "    \"product_id\",\n",
    "    \"event_type\"\n",
    ").orderBy(col(\"event_time\").desc())\n",
    "\n",
    "updates_df = (\n",
    "    oct_df\n",
    "    .withColumn(\"rn\", row_number().over(window_spec))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Step 7: Check count before merge\n",
    "before_count = spark.read.format(\"delta\").load(delta_path).count()\n",
    "print(\"Before merge count of rows:\", before_count)\n",
    "\n",
    "# Step 8: Perform MERGE\n",
    "delta_table.alias(\"t\").merge(\n",
    "    updates_df.alias(\"s\"),\n",
    "    \"\"\"\n",
    "    t.user_session = s.user_session AND\n",
    "    t.event_time = s.event_time AND\n",
    "    t.product_id = s.product_id AND\n",
    "    t.event_type = s.event_type\n",
    "    \"\"\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "# Step 9: Check count after merge\n",
    "after_count = spark.read.format(\"delta\").load(delta_path).count()\n",
    "print(\"After merge count of rows:\", after_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d76c50-0583-4c0c-8fe8-73fd5b51794d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# a) Show Delta history\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{delta_path}`\").show(truncate=False)\n",
    "\n",
    "# b) Query an old version by version number\n",
    "version_0_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(\"Number of rows in version 0:\", version_0_df.count())\n",
    "\n",
    "# c) Query as of a timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: all rows as of Jan 1, 2026\n",
    "yesterday_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2026-01-12 15:20:23\") \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(\"Number of rows as of timestamp:\", yesterday_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d01e1f-a45c-41ba-aab7-e0f174c25415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimize Delta table with ZORDER by columns often queried\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY (event_type, user_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768b656d-133c-4ed9-835d-8c6ed8b5437f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "VACUUM events_table RETAIN 168 HOURS\n",
    "\"\"\")  # 7 days\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8587327753182358,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Day - 5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}